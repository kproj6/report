\documentclass[11pt,a4paper,titlepage,oneside]{report}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc} % input encoding is UTF-8

\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{subcaption}
\usepackage{color}
\usepackage[unicode,pdftex]{hyperref}
\usepackage{xcolor}
\usepackage{longtable}

\begin{document}

% Title page %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{img/logo_NTNU.png}\\
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{img/logo_SINTEF.jpg}
\end{subfigure}
\end{figure}

\begin{center}
{\LARGE \textbf{TDT4290 - Customer Driven Project}}
\vfill
{\Huge \textbf{Ocean forecast}}

\vspace{12pt}
{\LARGE \textbf{SINTEF}}

\vspace{30pt}
{\LARGE \textbf{Final report}}
\vfill
{\LARGE \textbf{Autumn 2014}}
\end{center}
\vfill
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} l l}
\textbf{Group 6} & \textbf{Advisor} \\
Arve Nygård & Gleb Sizov \\
Anders Smedegaard Pedersen & \\
Emil Jakobus Schroeder & \\
Hans Kristian Henriksen & \\
Marco Radavelli & \\
Ondřej Hujňák & \\
Ruben Håskjold Fagerli & \\
\end{tabular*}

\end{titlepage}

% Empty page %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\thispagestyle{empty}
\mbox{}
\newpage

% Abstract %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
In the choice of location, as well as in the daily operations of fish farms, both historical data and up to date projections on sea currents are vital. SINTEF has given the task of visualizing data on ocean currents, temperature, salinity and other variables in areas around fish farms, to be used by stakeholders in the sea farming industry. 

Our group of seven students have used the fall semester at the Norwegian University of Science and Technology to do a full scale software development process in order to solve this task.

The group has written a pre study, were systems that are currently in use have been investigated, and technologies that could be used for custom built solutions have been evaluated. Using the Scrum methodology, the group has worked to develop both a front and back end prototype of such a system.

The prototype developed is able to read NetCDF files from SINTEFs simulations, and visualize this dynamically at the users discretion. This includes handling files that overlap both in area and resolution. 

Although the solution is a prototype, and not ready for full scale deployment, it proves that developing a custom system is within reach, and should be considered.

\end{abstract}

% Signatures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}
\begin{center}
{\large \textbf{Trondheim, \today}}\\
\vspace{2.5cm}
\begin{tabularx}{\textwidth}{@{\extracolsep{1cm}} X X }
\dotfill & \dotfill \\
~Arve Nygård & ~Anders Smedegaard Pedersen \\[1cm]
\dotfill & \dotfill \\
~Emil Jakobus Schroeder & ~Hans Kristian Henriksen \\[1cm]
\dotfill & \dotfill \\
~Marco Radavelli & ~Ondřej Hujňák \\[1cm]
\dotfill & \\
~Ruben Håskjold Fagerli & \\[1cm]
\end{tabularx}
\end{center}

% Table of contents %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
\addtocontents{toc}{\protect\thispagestyle{empty}}

% List of figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\listoffigures
\addtocontents{lof}{\protect\thispagestyle{empty}}

% List of tables %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\listoftables
\addtocontents{lot}{\protect\thispagestyle{empty}}

\pagenumbering{arabic}
\setcounter{page}{0}

% Main body %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}
\section{TDT 4290 - Customer driven project}
The task is set forth in the subject TDT 4290 - Customer driven project at the Norwegian university of science and technology. The goal of the course is 
\begin{quote}
(...)to give the students a practical experience of carrying out all the phases of a typical customer guided IS/IT-project. \cite{TDT4290:Intro}
\end{quote}
The subject divides the students into random groups, and assigns each group an assignment. The assignments are real problems that businesses needs solved. 

Although the assignment is to follow the entire process of an IT-project, the focus is on the earlier phases of a project. Thus, an important part of the assignment is the work leading up to the implementation phase. There is obviously also an important focus on the implementation itself. Maintenance is however left out of the scope of the projects.

\section{Customer}
The customer is SINTEF Fisheries and Aquaculture (SINTEF Fiskeri og havbruk AS). 
SINTEF was founded in 1950 and it is nowadays organized into 8 divisions. The division of Fisheries and Aquaculture was founded in 1999 and represents technological expertise and industry knowledge in the utilization of renewable marine resources. Under the vision \"Technology for a better society\" it works for a knowledge-based bio marine industry. Its goal is to meet market demands for technological research and development on renewable marine resources.

\section{Ocean currents}
The study of ocean currents allows for predicting what conditions can be expected in an area, both in short and long term. Historical data can be used to asses how an area will be exposed to temperature and salinity changes. Up to date projections are crucial for day to day operations like feeding and lice removal. It can also be used to predict the spreading of lice from one farm to another.

This makes the most relevant users of the system fish farmers, either in the planning or operation phase of the farm. The system is also relevant to anyone conducting ocean research in the given areas, and other stakeholders that can use information on the condition of the sea in their work.

\section{Assignment and scope}
The client, SINTEF Fisheries and Aquaculture, has given us the project labeled ”Ocean Forecast”. The task is to improve SINTEFs existing system of storing and retrieving data, as well as the way the data is presented to the end user. 

In agreement with the customer, the group has defined the following scope of the assignment:

\begin{quote}
Make a pre-study that examines what technology can be used to replace the current collection of PDFs, and instead serve the end user with custom visualizations on a web page. Follow the study with an implementation of a proof of concept system that is able to read from a collection of NetCDF-file, and present data to the end user based on custom chosen variables. We will ignore any bottlenecks that may exist in SINTEFs systems, and assume that they are fixable. Our end report will contain suggestions for further development and analysis.
\end{quote}

\section{Group resources}
The group consists of seven people with diverse skills. In the starting phase of the project, the group the different skills, and used this to distribute tasks, and areas of responsibilities. 

The project is set to run from August 28th until November 20th, for a total of 85 days, or 12 weeks and 1 day. As with all subjects given at NTNU, a student is expected to work 12 hours for every 7,5 study points. For this course, this amounts to 24 hours of work every week. Due to the fact that the presentation is given the week before the end of the semester, the course compendium sets the required work load to 25 hours a week. 

For our group, this amounts to about 2100 work hours in total. Every week, the group has 6 hours of meeting time, and is required to hand in documents for the advisor meeting. An estimate of 4 hours is given for the production of these documents. Given this, approximately 1260 hours can be dedicated to solving the customers problem.

\begin{equation}
Total\:work\:hours = 25\:\frac{hours}{week} * 12\:weeks * 7\:people \nonumber
\end{equation}

\chapter{Planning}
This section will give an overview of our plans for the project. It will include time estimation, group organization, risk management and quality assurance.

The name of the project and the scope of the assignment has already been introduced in the previous section.
\section{Project stakeholders}
The stakeholders of this project are the following:
\begin{description}
\item[Project owner (customer)] SINTEF Fisheries and Aquaculture
\item[Supervisor (from NTNU)] Gleb Sizov
\item[Examiner] External examiner, nominated by the direction of the course TDT4290 - Customer Driven Project
\item[Team members (students)] 
\end{description}

\section{Background for the project: software system development}
The customer owns a working solution, running on its own servers, for the delivery and analysis of marine tracked and predicted data, in form of maps with overlays and charts.
The background for this project is to optimize the actual, slow and memory-consuming solution, with a new one, and/or creating new front-end application using those data, to cover more use-cases.

\section{Measurement of project effects}
Some predefined criterias must be met in order to consider out project a successful one. Our product must be working according to the requirements and pass all tests in order to be a success. All requirements prioritized high and medium must be fulfilled. Requirements that have low priority are optional, but should be implemented if there is sufficient time.
In our case, quite open goals were requested by the customer, and no precisely quantifiable measurements were defined.
If working on improving the actual solution the measurements should be:
\begin{itemize}
\item The time it takes to visualize data for the end user should be “reasonably low” even with slow internet connection (like from Chile) or from a mobile device
\item The same displayed features as the actual working system are still available
\end{itemize}
If working on new use cases, the measurements should be:
\begin{itemize}
\item The same displayed features as the actual working system are still available
\item New features (such as mobile app and new charts) are added in order to cover more use-cases 
\end{itemize}
In both cases, an open source based solution is considered to be better than a commercial one.

\section{General terms}
The choice implementation language, platform and tools to use are up to us. The requirement is that the solution should run on a server.
There are non-functional requirements, in terms of improving performance and storage consumption, that should be accomplished.

\section{Planned workload}
Group members will aim to work 24-25h on average per week, for a total amount of 12 weeks. Group members can distribute the workload among days according to their lecture timetibles, as long as deadlines (including sprints deadlines) are met.
Our group consists of 7 people, so that the weekly average amount of workload is globally around 168-175h, for a total of 2016-2100 h.

\section{Life cycle model}
The group has chosen to use the Scrum methodology for the project. This is an agile method that focuses on incremental development. The customer is heavily involved in the development process, and is given frequent updates and demos of the progress of the team. 

\subsection{Scrum cycle}
\begin{figure}[h]
\begin{center}
\includegraphics[height=150px,width=340px]{img/Scrum.jpg}
\caption{Scrum cycle}\cite{ScrumCycle}
\label{fig:gantt}
\medskip
\small
\end{center}
\end{figure}

In the scrum methodology, there is a set way to conduct a development process. The process starts by laying out a product backlog. This is a overview of what needs to be done to complete the entire product. It does not have to have very high detail, but it should be detailed enough to give a clear idea of how the team should proceed. 

After the product backlog is written, the first sprint is planned. In this planning meeting, the team estimates the time needed for each task, and takes on the tasks needed to complete the goal set for the sprint. This process is done in cooperation with the customer. The customer may set the sprint goal, or get more directly involved in the selection of tasks. 

When the team has chosen the tasks to be completed, the sprint is locked, and new tasks may not be added by the customer. This gives the development team a noise free environment, and allows them to focus on the task ahead. 

In the sprint cycle, which typically lasts for around 3 weeks, the team has daily meetings to update on progress and challenges. This meeting is lead by the scrum master. The daily meeting is known as a standup, as the participants are expected to stand during the meeting. Every person is given about two minutes to give an update of their work.

When the sprint is completed, the customer is given a demo of what the team has completed. This should be all that was agreed on in the sprint planing meeting. The customer is now free to give new priorities to items in the product backlog before the team picks new tasks for the next sprint. 

\subsection{Scrum adaptions}
As our development project is part of a university course that runs in parallel with other subjects, it is impossible to follow the standard scrum process. Given this, the group made some choices that diverges from the standard scrum methodology.

\begin{itemize}
\item Meetings are not conducted every day, but rather twice a week. This fits better with the schedule of the group members, and takes into consideration that there is no demand for work to be done every day. 
\item Sprint length has been chosen as one week. This is short compared to the Scrum standard, but it provides the group with a motivational pressure, as well as making the sprints easier to estimate. 
\item Product demos are set to every other week. This means that there will be sprints that are not presented to the customer. In choosing this, the group is able to have sprints that focus on the internal demands set forth by NTNU, as well as making more progress between customer meetings.
\end {itemize}

These changes make the Scrum method well suited for use in the project. Although it is important to try to follow well tested methodologies to the letter, one can not allow them to block progress, or disturb the workflow of the team. When in a special work situation, as the group is in, this type of adaptions are necessary for the efficient use of any development methodology. 

\section{Schedule of results}
We are using a Scrum-like project management framework, and working prototypes are expected to be produced at the end of each sprint. Requirements have been prioritized by the customer with numbers from 1 to 100. The requirements for our project are inevitably overlapping, and almost all the requirements must be accomplished in order to produced a working solution, to satisfy the basic project goal requested by the customer.

\section{Concrete project work plan}
This section describes the specific layout of the project. This project follows the Scrum methodology with some variants introduced, as discussed above in the document. The first five weeks were spent planning, pre-studying (which included a preliminary implementation of the critical part of the system), and writing the requirement specification. As a common practice, the project was divided into sprints. We decided to make each sprint lasting one week. At the end of the last sprint all the requirements should be accomplished.
\subsection{Phases}
We decide to the Scrum methodology, so that Scrum sprints characterize the phases of the project.
The following are the phases that can be distinguished in the project:
\begin{itemize}
\item Initiation phase (pre-planning)
\item Project Planning
\item Project Prestudy Sprint (Sprint 0)

\item Sprint 1
\item Pre-delivery of report (17th October)

\item Sprint 2
\item Sprint 3
\item Sprint 4
\item Sprint 5
\item Sprint 6

\item Finish report
\item Prepare for presentation
\item Final Delivery and demonstration
\end{itemize}

In figure \ref{fig:gantt} the Gantt diagram for our project is presented:
\begin{figure}[h]
\begin{center}
\includegraphics[height=130px,width=440px]{img/gantt.png}
\caption{Gantt diagram}
\label{fig:gantt}
\medskip
\small
\end{center}
\end{figure}

\subsection{Activities}
\begin{description}
\item[Pre-planning] During the pre-planning activity, the group members know each other and understand the task. There are also the first meetings with the customer and with the avisor.
\item[Pre-study and planning] The pre-study and planning activity is the stage where the actual work on the project begins. We explpore solutions and we determine which technologies will be used and how the product will be realized.
\item[Documentation] The documentation activity represents time spent documenting the work effort, including implementation, research etc., and administrative tasks like status reports and documents for the meetings.
\item[Coordination] The coordination activity is accomplished throughout the project, and consists of activities to coordinate the work, such as meetings, internal emails, calls and messages.
\item[Implementation] The implementation activity consists of the implementation of the system. This includes the programming of both the back end and the front end part.
\item[Testing] The testing activity represents time spent testing the system. This includes integration testing, unit testing, functional testing and scenario-driven testing.
\item[Presentation] The presentation activity is the final presentation of the system and the delivery of the report.
\end{description}

\subsection{Milestones}
We defined some milestones in order not to create delays in the progress of the project.
There are some milestones defined by the course structure, and we decided to add some more "internal" project milestones in order to avoid delays.
The following are the defined milestones:
\begin{description}
\item[October 6th] Pre-study phase to be completed
\item[October 17th] Pre-delivery of report (defined by the course coordinators)
\item[November 17th] Project report to be completed
\item[November 20th] Final presentation day (defined by the course coordinators)
\end{description}

\subsection{Lectures}
In this projects there have been a number of guest lectures, and we have made sure that every lecture were attended by at least one member in the team.
The "Group Dynamics" lecture was mandatory and everyone in the group attended it.
The other lectures, specifically, have been:
\begin{itemize}
\item "How to sell in large application projects", Thomas B. Pettersen - Computas (01.09.2014)
\item "Scrum, agile development method", Torgeir Dingsoyr - SINTEF (02.09.2014)
\item "Estimation, agile/practical project work", Fredrik Bach - BEKK, (02.09.2014)
\item "Project management", Stian Mikelsen - Bearingpoint (15.09.2014)
\item "Technical Writing in English", Stewart Clark - NTNU (24.09.2014)
\item "Sales techniques with exercises in groups", Morten Selven - Mikos (01.10.2014)
\end{itemize}

\section{Project Organization}
\subsection{Organizational diagram of how the group is organized}
In figure \ref{fig:organizational-structure}, the structure of the group is shown. 

\begin{figure}[h]
\begin{center}
\includegraphics[height=260px,width=328px]{img/tdt4290_group_6_organizational_structure.png}
\caption{Organizational diagram.}
\label{fig:organizational-structure}
\medskip
\small
The arrows indicate the bi-directional communication. The dotted arrows denotes the preferred way of communication between components. Note that the layout is not hierarchical and it is arranged this way only to better fit on the printed page.
\end{center}
\end{figure}

\subsection{Roles and corresponding responsibilities}
We defined and diveded roles and responsibilities among group members. We agreed the distribution of the responsibilities by the voluntary choice of the role for each component of the group, and internal unanimous agreement.
We conveyed the following roles:
\begin{description}
\item[Scrum master] His task is to make sure that we meet our goals, he is the first person who talks to the customer and to the advisor in the meetings, make sure that the groups meetings have a structure and that they finish on time. According to the scrum methodology, the scrum master also has to supervide the scrum pocess (control that is respected), remove eventual impediments to the team's work, protect team from distracting influences, and make sure that the team is on-task.
\emph{Role assigned to: \textbf{Ondřej Hujňák}}
\item[Customer contact] His task is to arrange customer meeting, forward customer emails to the group if needed, and ensure that all the needed communication with the customer is done.
\emph{Role assigned to: \textbf{Marco Radavelli}}
\item[QA and testing responsible] Ensure that the implementation fulfills the requirements, design test plan, define and ensure that the Quality Assurance standard is followed during the project.
\emph{Role assigned to: \textbf{Emil Jakobus Schroeder}}
\item[Documentation responsible] Supervise the structure and the content of all the documents (report, status reports, agenda and minutes of meetings, pre-study report). Main responsible to take notes during meetings. 
\emph{Role assigned to: \textbf{Hans Kristian Henriksen}}
\item[Advisor contact] Arrange advisor meetings, send documents (status report, agenda and minutes of last meeting) to the advisor before each meeting.
\emph{Role assigned to: \textbf{Hans Kristian Henriksen}}
\item[Front-end leader] Supervise the front-end architecture and implementation, and coordinate the front-end developer team
\emph{Role assigned to: \textbf{Anders Smedegaard Pedersen}}
\item[Back-end leader] Supervises the back-end architecture and implementation, and coordinate the back-end developer team
\emph{Role assigned to: \textbf{Arve Nygård}}
\item[System architect] Make sure that there is consistency between requirements, design and implementation, and that the design is feasible and reasonable.
\emph{Role assigned to: \textbf{Ruben Håskjold Fagerli}}
\end{description}

\subsection{Weekly schedule}
We decided to adopt a scrum-like model of software development, with internal group meetings twice a week, weekly meetings with the advisor and meeting with the customer when needed (typically every other week). The schedule is defined as follows:
\begin{itemize}
\item Mondays 2-3 pm - Advisor meeting
\item Mondays 3-4pm - Team meeting
\item Thursdays 12-2pm - Team meeting
\end{itemize}
For weeks 37 and 38 the advisor meeting will be Thursday 4pm.

We decided to use the academic quarter for internal meetings, but to start the meeting at the time conveyed sharp for advisor meetings.

\section{Risk assessment}
  \begin{longtable}{p{0.7cm} p{2.5cm} p{0.7cm} p{0.7cm} p{6.5cm} }
  \caption[]{Risk assessment}\\
  \multicolumn{1}{p{0.7cm}}{ID} &
  \multicolumn{1}{p{2.5cm}}{Problem desc.} &
  \multicolumn{1}{p{0.7cm}}{Prob.} &
  \multicolumn{1}{p{0.7cm}}{Sev.} &
  \multicolumn{1}{p{6.5cm}}{Action}
  \endhead

  \caption[Risk assessment]{} \label{riskAss} \\
  \hline
  \multicolumn{1}{p{0.7cm}}{ID} &
  \multicolumn{1}{p{2.4cm}}{Problem description} &
  % \multicolumn{1}{p{0.8cm}}{\parbox[t]{0.8cm}{Proba-bility}} &
  % \multicolumn{1}{p{0.8cm}}{\parbox[t]{0.8cm}{Cons-equens}} &
  \multicolumn{1}{p{0.7cm}}{Prob.} &
  \multicolumn{1}{p{0.7cm}}{Sev.} &
  \multicolumn{1}{p{6.5cm}}{Action}
  \endfirsthead
  
  \hline
  \multicolumn{5}{r}{{Continued on next page}} \\
  \endfoot

  \hline \hline
  \caption[Risk assessment]{Here "Prob." stands for "Probability" and "Sev." stands for "Severity"}
  \endlastfoot

  \hline
%\begin{tabularx}{\linewidth}{lXllX}
R1 & Personal Conflict & M & M & Might affect the quality of the work, motivation and time required. Probability can be reduced by ensuring communication within the group, and democratic and motivated decisions. \\ \\ \hline
R2 & Other courses’ assignments problems & H & M & Time for some students could be less than planned during a few weeks, due to intensive-demanding assignments of other courses. Mitigated by planning, frequent workload balancing and contributing with extra work in weeks without as intensive demands. \\ \\ \hline
R3 & Conflicting schedules & H & M & Reduced by planning meeting-times and communication channels for times between group meeting. Also, accepting that we can't meet every day and that all group members do not always need to be present if tasks of the person allows. \\ \\ \hline
R4 & Solution is overcomplicated & M & M & Probability reduced by talking within the group and customer in order to find possible ideas for a simpler solution. \\ \\ \hline
R5 & Technical problems and difficulties & M & H & To reduce the probability: make a detailed study (and prototypes where possible) during the pre-study phase. To reduce the consequence: find the person/people within the group which are more comfortable with those new technologies and assign them that task if it is a particular task. Knowledge transfer within the team is also a way to reduce the consequences of this risk. Keep the customer informed. \\ \\ \hline
R6 & Room reservation & H & L & Will delay work. Avoid by making sure we have a regular room (talk to advisor \\ \\ \hline
R7 & Data loss & L & H & It can be limited by introducing a backup strategy. \\ \\ \hline
R8 & People don’t complete an assigned task & H & M & Consequence reduced by additional work for other group members in order to try to complete the task within the deadline. Talk within the group to find a compromise. Tampered by assuring daily communication among team members. \\ \\ \hline
R9 & Illness & H & M & Probability cannot be reduced, but consequence can be tampered by temporary increases the work for other members of the group. \\ \\ \hline
R10 & People come late, or not come, to the meetings & H & M & Probability reduced by defining clear rules and schedules for the meetings. Be 5 minutes early for the meetings with the customer. \\ \\ \hline
R11 & People cannot find the room for the meeting or come in the wrong date & H & M & Probability reduced by creating a calendar shared by the group members and constantly updated. Ensure communication within the team. Reserve the same room and keep the same schedule for meetings. \\ \\ \hline
R12 & Cannot complete the product backlog or sprint backlog & M & H & Reduced by making sure to follow the priority of the tasks. Talk with the customer. \\ \\ \hline
R13 & Mistakes in the documentation or in the final product & M & M & Probability reduced by ensuring that every deliverable is reviewed and checked by at least a different person than who has written it. Make sure to dedicate resources for software testing and review of documents. Use the commenting and collaboration tools (or features inside tools) to help reviewing. \\ \\ \hline
R14 & Someone’s computer get out of order & L & M & Probability cannot be reduced. Consequence can be reduced by increase work for other team members, and leave to that person documentation work, that doesn’t need a specific tool to be installed, so that he can do it on the computers at university for instance. \\ \\ \hline
%\end{tabularx}
\end{longtable}

\chapter{Tools and technology}
\section{Documents}

  \subsection{\LaTeX}
  \LaTeX~is a typesetting system and document markup language that became standard for scientific documents. It is easily expandable by thousands of different packages and can handle all aspects of scientific papers.

  We have chosen to use \LaTeX~for our report for two main reasons - \LaTeX~sources are easily readible and, because they are simple text files, they can be easily versioned by various version control systems. Second reason was focus on content, not on form. In \LaTeX~sources there is only very little information about exact view of the page. \LaTeX~itself during compile time chooses the best position of elements so it complies with all typographic norms.

  We have created a template in our shared space together with a bibliography file. Everyone could then write his sections in an environment that suited him the best while the current state of the report was always available to all members.

  \subsection{Google Docs}
  Google Docs \footnote{\url{https://docs.google.com}} is a web based office suite including a text editor, a spreadsheet program and a presentation program. All files created in these programs can easily be shared with colaborators. By sharing files the colaborators get access to view and edit the files. Editing and commenting on other's work. We decided to use Google Docs for all documents that did not require the advanced typesetting of \LaTeX~so we had a common platform for such documents.

\section{Project management}
  \subsection{Trello}
  Trello \footnote{\url{http://www.trello.com}}is a web-based collaborative project management tool originally made by Fog Creek Software (New York, USA) \footnote{\url{http://www.fogcreek.com/}}. 
It's based on the Kanban method which has first implemented by Toyota in 1953 to be used in car production. It has since been modified to be used in several different industries. 
David J. Anderson formulated a model based on Kanban for knowledge based work, specifically software development, where the team work incrementally pulling work from a queue \cite{da2004}. 
We use Trello for organizing our tasks into four states: "To do", "Doing", "Blocked" and "Done". The default state is "To do" and by changing the state of a task everybody in the group knows what needs to be done, what is being worked on, what tasks are dependent on other tasks or factors and what tasks are done.
This is a simple yet efficient way of managing tasks. We chose Trello for the ease of use and thus takes very little time to learn to use. In comparison a system like Jira \footnote{\url{http://https://www.atlassian.com/software/jira}} has more features that might have been useful but it takes more time and effort to learn. Therefor we we opted for Trello.
Trello is a freemium webservice which means that it is free to use but additional support and features can be accessed if you pay a fee. As we only needed the standard functions we used the free version.
  \subsection{Slack}
Slack is a webbased team communication tool founded by Stewart Butterfield. It offers text chat in different channels and integration with a number of different popular services used by development teams \footnote{\url{https://slack.com/integrations}}. This was useful to us since we needed to share information that might be more relevant to specific team members and also to have a single means of communication. Using Slack's integration with Trello and Github we would get notified when there where changes on these platforms aswell. Slack's posibillity to share files, or link to files on Google Documents also came in handy.

\section{Version control}
  \subsection{Git}
  Git is a distributed version control system developed in 2005 by Linus Torvalds and Linux development community \cite{ProGit}. Git was made to be small, fast and easy to use especially for code management as it's main purpose was versioning of Linux kernel source code. Nowdays is Git one of the most used versioning controls systems in software development thanks to it's open linence and powerfull features.

  We have chosen Git because some members already know it and are able to work efficiently with it. Another advantage is easy branching and distributed architecture that allows you to work offline. 
  
  We have created an organization on GitHub\footnote{\url{https://github.com}} with multiple repositories for different separate parts of our work - reports, server sources, client sources. We have chosen GitHub because it is well known git hosting server that offers advanced features and stability. Moreover some members already had accounts on GitHub and were familiar with interface, which shortened time needed to setup a working environment.

\section{Programming languages}
  \subsection{Java}
  Java is a popular programming language developed by a team led by James Gosling at Sun Microsystems in 1991. It's full-featured , general purpose language capable of developing robust mission-critical applications \cite{liang}. This and the fact that everybody in the group had at least basic knowlegde of Java led to the dessicion that we would use Java for our back-end application.  
  \subsection{JavaScript}
  As stated by Davis Flanagan i JavaScript: the definitive guide "Javascript is part of the triad of technologies that all Web developers must learn". He continues to note that JavaScript specifies the behaviour of the web page \cite{fd11}. Along with specification of HTML5 and ECMAscript 6 (the standard name of JavaScript) the posibilities of what you can achieve with JavaScript has greatly improved. Since our assignment was to create a web based solution it seamed natural to use JavaScript as part of the solution. This was further supported by the excistence of open source libraries designed to make interactive maps which was relevant for our assginment.

\chapter{Pre study}
In this chapter, we present the findings of the pre study. The pre study document was made as a separate document that was meant to be delivered to the customer independently of this report. Therefore, there are overlapping sections with the full report. These sections will not be given in this chapter, but are instead presented in their respective chapters of the full report. 

\section{Background}
To get an understanding of the customers needs, as well as studying different solutions, a pre study is conducted. From the course compendium:
\begin{quote}
The preliminary studies are vital for the group to obtain a good understanding of the total problem.
Here, you will have to describe the problem at hand. You should describe the current system and the
planned solutions (...).
\cite{TDT4290:Intro}
\end{quote}

\section{Current situation at SINTEF}
In this chapter we will explore the solution SINTEF is currently using, and the challenges and limitations it poses. After looking into this, we will describe the evaluation criteria that will be used to assess the alternative solutions the group has found. 
\subsection{Current system}
\begin{figure}[h]
\begin{center}
\fbox{\includegraphics[height=223px,width=396px]{img/region_interface_sinmod.png}}
\caption{The main interface for a region in SinMod}
\label{fig:sinmod-region-main-interface}
\end{center}
\end{figure}

The current system deployed at SINTEF serves their clients by providing access to a collection of more than 100 000 pre generated PDF files. These files contain information on currents, salinity, and temperature. The user may choose what information he\footnote{"He" should be read as "he or she" throughout this report} wants by selecting parameters in the drop down menus, see figure \ref{fig:sinmod-region-main-interface}.

\begin{figure}[h]
\begin{center}
\fbox{\includegraphics[height=223px,width=396px]{img/site_key_data.png}}
\caption{The key data a user is presented with when selecting a specific site}
\label{fig:sinmod-site-key-data}
\end{center}
\end{figure}

If the user chooses a specific site from the map or location drop down, he will be presented with key data for this area. This includes statistical information such as maximum current speed, average current speed and so forth, as well as geographical position, as given in figure \ref{fig:sinmod-site-key-data}.


\begin{figure}[h]
\begin{center}
\fbox{\includegraphics[height=300px,width=300px]{img/site_graphs.png}}
\caption{Some of the graphs presented to the user when selecting a specific site}
\label{fig:sinmod-site-graphs}
\end{center}
\end{figure}

The system will also present a set of pre defined graphs, including current roses, tidal ellipsis and vertical profiles. The graphs are given for standard attributes (e.g depth 2 meters), and for some of them, there is an option of downloading a PDF containing graphs for other values of the given attributes. An example is shown in figure \ref{fig:sinmod-site-graphs}


\subsection{Challenges}
As the PDFs are pre generated, there is a clear limitation to what information the user may request. If a user wants to know, for example, the connection between salinity and current speed at a given location, the user must download two different PDFs and manually compare these. 

The graphs given for a specific location are only given for limited values of the critical attributes. If we look at the current rose, it is presented for a depth of 2 meters. If the user is really interested in the current rose for 10 meters, he has to download the PDF containing all possible current roses. 

The same is true for the maps that can be generated for a specific site. The user may choose period (a single month may be selected), and one of the five variables. This gives the user a PDF with one map for each depth that can be calculated. 

For a user who knows what data is interesting, this is a complicated and data heavy way of delivering information. The PDFs seems to range in size from 125kB to around 3MB, depending on what information is requested. The region maps are the absolute largest in file size, ranging from 1 to 3MB, while the files containing the current roses are quite small, in the 100-200kB range. 

On a computer with broadband connection, the size of the files is not very problematic. For these users, the biggest challenge is the fact that the user can not specify what kind of data they want plotted, and have to look through quite a lot of pages to get the information needed. For a user on a low bandwidth connection and/or on a mobile device, the size of the files is a more pressing problem. On an EDGE connection the theoretical best download time for a 3MB file is 62,5 seconds at 384kbit/s \cite{3gpp.com}.

\subsection{Evaluation criteria}
SINTEFs main goal with this project is to be able to rid themselves of the PDF store, and generate the information on demand. This will make for a much more flexible system, where it will be possible to add new graphs and functionality quickly. For the customers, it will make it possible to request more customised graphs and plots.

SINTEF has presented the group with several goals they wish the new system to fulfil:

\begin{itemize}
\item The system should generate the graphs and maps directly from the NetCDF files on user request.
\item The user should be able to select several variables for one plot.
\item The system should be usable on low bandwidth connections.
\item The system should be usable on mobile devices.
\item The system should be easy to expand with new functionality.
\end{itemize} 

With these goals in mind, the group started looking into different technologies that could be used to make such a system.

%%%
\section{Other production solutions}
The group has used the first part of the project investigating what solutions would best suit SINTEFs needs. In the chapter we present the different solutions we have found, along with an assessment on how each solution is rated in accordance to the evaluation criteria. 

%Existing tech - Italian solution
\subsection{Adriatic Forecasting System}
\emph{Link: \url{http://oceanlab.cmcc.it/afs/}} \\%make sure to use //
  The solution, by the Operational Oceanography Group Italy and cmcc Ocean-Lab, can display temperature, salinity, currents, sea surface height, wind stress and heat flux. It allows the user to choose date, region and depth as search filters, and uses PNG overlays on a Google map. The PNGs are retrieved from cmcc's own server.
  Although it graphically looks quite nice, it can be seen that some tiles are not precisely overlapping. In addition, sometimes it is needed to refresh the page because the application doesn't load properly. PNG layers are displayed without a specific JS library, and the JS code is quite complex compared to other existing solutions using libraries. Therefore this solution is not well re-usable.
\\ \emph{Overall rating: \textbf{Ok}}

\subsection{Danish Centre of Ocean and Ice}
  \emph{Link: \url{http://http://ocean.dmi.dk/anim/index.uk.php }} \\%make sure to use //
    The solution by the Danish Center of Ocean and Ice can display temperature, salinity and current, which are the most important factors of the SINTEF simulation. This said, it lacks the ability to choose depth and specify a date interval. The data is shown as static PNGs, thus the map is not interactive. There is, on the other hand, a possibility to choose different geographical areas with the highest level of detail around Denmark. This is on the same level as SINTEF's existing solution.
  \\ \emph{Overall rating: \textbf{Bad}}

  \subsection{Fisheries and Oceans Canada}
  \emph{Link: \url{http://www.tides.gc.ca/eng}} \\%make sure to use //
    The solution of the Canadian government resembles the Danish one. It is possible to choose a geographical area on a static map. By choosing an area you get the opportunity to choose a smaller, more specific area. The big difference is that all data is presented as text in tables, thus making it less convenient and intuitive to use.
  \\ \emph{Overall rating: \textbf{Bad}}

  \subsection{Ocean viewer}
  \emph{Link: \url{http://www.oceanviewer.org}} \\%make sure to use /
    Ocean Viewer is a pilot project of the Marine Environmental Observation Prediction and Response Network (MEOPAR) of Canada. It gathers data from different sources and displays it as PNGs overlayed on a map. You can select different geographical areas on a customized Google Map and different data from a menu (temperature, salinity and others). Like the Danish solution the PNGs can be shown in sequence to show changes over time.
  \\ \emph{Overall rating: \textbf{Ok}}

  \subsection{Sea temperatures and Currents - Bureau of Meteorology}
  \emph{Link: \url{http://www.bom.gov.au/oceanography/forecasts/}} \\%make sure to use //
    The Australian Bureau of Meteorology has a solution that is very similar to the other national agencies. You can choose a geographical area on a static map. Here as well, the data is visualized with images overlayed on a static map, with the possibility to loop through the images to show changes in the data over time.
  \\ \emph{Overall rating: \textbf{Ok}}
  
  \subsection{yr.no Map Service}
  \emph{Link: \url{http://yr.no/kart}} \\%make sure to use //
This solution presents the user with a conventional map interface. On the sides and top there are menus for selecting which variable and timestep should be displayed. The user is only allowed to select a timestep about 8 days from the current time. Interesting variables include sea temperature, salinity and sea currents, each has it’s own . There is no way to select depth, and all data seems to be for the surface values. The chosen variable is added as an overlay of PNG tiles using OpenLayers. The tiles are fetched using WMS from a norwegian meteorological service server. On the map there are several measurement stations displayed, that give information when clicked. In addition, clicking anywhere on the map displays several plots predicting the next two days of weather for that spot.
  \\ \emph{Overall rating: \textbf{Ok}}

\section{Back-end}

\subsection{GeoServer}
Does not support NetCDF natively. There exist a community plugin that enables you to read from NetCDF files, but the support seems very shifty. GeoServer does not seem to have support for more than one file and metadata appears difficult to extract. Due to this factors, the system was deemed to not to meet the needed criteria.

\subsection{THREDDS}
\begin{quote}
The THREDDS\footnote{\textbf{Th}ematic \textbf{R}ealtime \textbf{E}nvironmental \textbf{D}istributed \textbf{D}ata \textbf{S}ervices} Data Server (TDS) is a web server that provides metadata and data access for scientific datasets, using OPeNDAP, OGC WMS and WCS, HTTP, and other remote data access protocols. \cite{TDS:Web}
\end{quote}

  Except supporting multiple data access protocols, TDS is able to virtually aggregate multiple NetCDF files to one dataset that can be used for queries, such as selecting a region and sending it's data in specified format. Dataset configurations are done via NcML\footnote{\textbf{N}et\textbf{C}DF \textbf{M}arkup \textbf{L}anguage}, which is a dialect of XML.

  Although TDS seems to support everything that is needed for this project, it's installation, and especially configuration for agreggation and special needs, is not trivial. Moreover there are doubts about speed and dealing with serving a range containing large quantified points. One advantage is that SINTEF currently have a TDS running and configured, so we won't need to configure it from scratch, and SINTEF employees probably have experience and knowledge about setting it up, something which the group lacks.

  \subsection{Custom solution}
  An alternative is to write a custom back-end from scratch.
  Advantages of this are:
  \begin{itemize}
  \item Easy deployment - written as a single service that you just launch.
  \item Load balancing - Easily scale outwards: The servers can be put behind a load balancer. 
  \item Speed - No overhead for unused features.
  \item Code clarity - No overhead for unused features.
  \end{itemize}
  A skeleton for the whole server is in place. Below is a list of features that the group feels are within reach in the project period. 
  \begin{itemize}
  \item Indexing and file selection.
  \item Projection (Mapping between latitude/longitude and file indices)
  \item Filtering (Reducing the result set before reading the file)
  \item Output
  \item Rendering to image
  \item Rendering to GeoJSON
  \end{itemize}

  Performance seems to be very good at the current level of implementation. The only major potential bottleneck the group sees is reading files from a hard-drive. (All testing has been done on SSDs). This bottleneck will however be independent of backend solution.
  
  \subsection{MapTiler}
  \emph{Link: \url{http://www.maptiler.org/}} \\%make sure to use //
    MapTiler is an application for online map puplishing. It makes it possible to create tiles that can be overlayed over other maps like Google Maps, Open Street Map and others. It's written in C/C++ and claims to be a lot faster than other existing solutions. A draw back seems to be that it's made for overlaying a pre generated directory of images rather than dynamic data like the ocean forecast data.
  \\ \emph{Overall rating: \textbf{Ok}}
  
    \subsection{ncWMS}
  \emph{Link: \url{http://www.resc.rdg.ac.uk/trac/ncWMS/}} \\%make sure to use //
    ncWMS is an open source, free to use, java server application. It was created to support interactive browsing of gridded four-dimensional netCDF data over the web. Clients will send request containing the wanted coordinates (latitudes, longitudes, depth and time), what variable is to be displayed, projection, format and size the response should be. ncWMS, which has been configured to read from datasets (for example sets of netCDF files or a THREDDS server), responds to a request with an image of the desired type. ncWMS adheres to the WMS specification (WMS 1.3.0 and 1.1.1 are supported).
    \paragraph{Configuration}
    ncWMS is mainly configured through a web interface, where you can add datasets and change server settings. A dataset can be: a single netCDF file, an OPeNDAP endpoint (a service provided by THREDDS), a NcML file or a glob aggregation (using wildcard characters like ). It is possible to configure which variables of a dataset to expose and how. Each dataset can also be set to automatically refresh at certain intervals. The server can be set to cache data, to reduce cpu load at the expense of memory and disk space.
    \paragraph{Aggregation}
    The glob aggregation or NcML work well with files that cover the same area, but contain different timesteps. To handle different areas and different resolutions it may be possible to use THREDDS/OPenDAP or NcML.
    \paragraph{Perfomance}
    Testing both local and publicly available ncWMS servers the performance seems good. When using the godiva2 browser based client or sending individual requests the server mostly responds quickly, with some idiopathic exceptions. Transmitting the map data to the client as png images should be a bandwidth-efficient way to do it, as well as moving the computation load away from the client. When starting the server application or adding a dataset, the server needs some time to load some information from the datasets. This only takes a few seconds, even for a few hundred GB of local netCDF files.
    \paragraph{Summary}
    ncWMS does a lot of what we need the back-end of our solution to do. It handles the extraction, downsampling and projection of the data, creating an image ready to be used in a map widget or on it’s own. It can only handle requests for a few kinds of plots. It is open source, and is free to use under a modified BSD license. To serve all the plots and data required it would need modification.
  \\ \emph{Overall rating: \textbf{Good}}

%%%
\section{Transmission Protocols}
  There are several standardized protocols to present map data from the server to the client in order to dinamically display layers on a map.
  In particular, two main kind of representations can be distinguished, and sometimes they're both supported by a single standard:
  \begin{description}
    \item[Vector based layers] Data is sent from the server to the client in a textual format, such as GeoJson
    \item[Image based layers] Data is sent from the server to the client as images, such as Jpeg, png and gif
  \end{description}

  The Open Geospatial Consortium (OGC) became involved in developing standards for web mapping after a paper was published in 1997 by Allan Doyle, outlining a \"WWW Mapping Framework\". The oldest and most popular standard for web mapping is WMS. However, the properties of this standard proved to be difficult to implement for situations where short response times were important. For most WMS services it is not uncommon to require 1 or more CPU seconds to produce a response. For massive parallel use cases, such a CPU-intensive service is not practical. To overcome the CPU intensive on-the-fly rendering problem, application developers started using pre-rendered map tiles. Several open and proprietary schemes were invented to organize and address these map tiles. 
  In order to reduce the performace problems of WMS, new standards have been defined:
  \begin{itemize}
    \item TMS
    \item WMS-c
    \item WMTS
  \end{itemize}
  
  \subsection{WMS}
    Web Map Service (WMS) is a standard protocol for serving georeferenced map images over the Internet that are generated by a map server using data from a GIS database. The specification was developed and first published by the Open Geospatial Consortium in 1999.
    A WMS server usually serves the map in a bitmap format, e.g. PNG, GIF or JPEG. In addition, vector graphics can be included: such as points, lines, curves and text, expressed in SVG or WebCGM format.
    The WMS standard allows flexibility in the client request enabling clients to obtain exactly the final image they want. A WMS client can request that the server creates a map by overlaying an arbitrary number of the map layers offered by the server, over an arbitrary geographic bound, with an arbitrary background color at an arbitrary scale, in any supported coordinate reference system.

  \subsection{TMS}
    Tile Map Service or TMS, is a specification for storing and retrieving cartographic data, developed by the Open Source Geospatial Foundation. The TMS protocol fills a gap between the very simple standard used by OpenStreetMap and the complexity of the WMS standard, providing simple urls to tiles while also supporting alternate spatial referencing system.
    TMS is most widely supported by web mapping clients and servers, and it is served as the basis for WMTS (the OpenGIS Web Map Tile Service OGC standard).

  \subsection{WMS-c}
    The WMS Tiling Client Recommendation, or WMS-C for short, is a recommendation set forth by OSGeo for making tiled requests using WMS. It is just a recommendation on using WMS properly in order to improve performance by caching data.
    This recommendation relies on two basic concepts to support this purpose: First, cachability of map imagery can be improved by using image tiles of fixed width and height, referenced to some fixed geographic grid at fixed scales. A valid tile request is one that conforms to the specification of fixed image parameters and geographic grid(s) for a given layer. By analogy, an invalid tile request is one that does not.
    Second, caching of HTTP GET requests is further made possible by constraining the URL parameters used in the request. This recommendation identifies the WMS GetMap parameters minimally needed for a client to request a valid tile.

  \subsection{WMTS}
    Web Map Tile Service (WMTS) is a standard protocol for serving pre-rendered georeferenced map tiles over the Internet. The specification was developed and first published by the Open Geospatial Consortium in 2010
    WMTS builds on efforts to develop scalable, high performance services for web based distribution of cartographic maps. To define this standard, similar initiatives were also considered, such as Google maps and NASA OnEarth. WMTS includes both resource (RESTful approach) and procedure oriented architectural styles (KVP and SOAP encoding) in an effort to harmonize this interface standard with the OSGeo specification.
    WMTS complements earlier efforts to develop services for the web based distribution of cartographic maps. The OGC WMTS provides a complementary approach to the OGC Web Map Service (WMS) for tiling maps. WMS focuses on rendering custom maps and is an ideal solution for dynamic data or custom styled maps. WMTS trades the flexibility of custom map rendering for the scalability possible by serving of static data (base maps) where the bounding box and scales have been constrained to discrete tiles. The fixed set of tiles allows for the implementation of a WMTS service using a web server that simply returns existing files. The fixed set of tiles also enables the use of standard network mechanisms for scalability such as distributed cache systems.

  \subsection{Summary}
    WMS-C is the best supported and most mature protocol, but it's a bit of a kludge overlayed on top of WMS to support tiles and it incurs some extra overhead from having to use world coordinate bounding boxes rather than tile coordinates.
    TMS is fairly mature, and is specifically designed for tiles, but is not an official OGC spec.
    WMTS is an OGC spec that is meant to replace TMS and WMS-C. It works purely in tile coordinates like TMS (although it computes them differently) but has some additional capabilities that weren't in TMS, like GetFeatureInfo. It's comparatively recent, but it is becoming more and more used, even if its implementations are less mature. It is also supported by OpenLayers.

%%%
\section{Frontend solutions}
  A central part of the product requirement is to display simulated data in a dynamic manner. There exists solutions to do just that, and javascript libraries than makes it relatively easy to create at front end solution of our own. In this section we will discuss and compare the most relevant of these in the context of our assignment and the product requirements. 
  \subsection{Custom made solution}
    A custom made solution has the general advantage that we can built it to specification and thus make sure it meets the requirements without having to deal with other peoples code base. Trying to customize an existing solution might be as much work as building something from the bottom. In the following paragraphs we will review and rate technologies we found relevant for building a custom front end.
    \subsubsection{LeafletJS}
    \begin{tabular}{|p{4cm}|p{8cm}|}
      \hline
      Home page: & \url{http://www.leafletjs.com} \\
      \hline
      Service functionality: & Creating mobile-friendly interactive maps. \\
      \hline
    \end{tabular}
    
    \paragraph{Introduction} \indent
    LeafletJS ("Leaflet" for the rest of the section) is an open source javascript library for creating mobile-friendly interactive maps. It's licensed under the \href{'https://github.com/Leaflet/Leaflet/blob/master/LICENSE'}{2-clause BSD License}, which makes it free to use in commercial applications as long as a credit is added somewhere in the user interface.
    Even though Leaflet is free to use it is dependent on a third party to provide the map tiles. These may not be free to use.

    \paragraph{Features}
    Leaflet has the features you'd expect from a modern interactive map. This includes paning with inertia, zooming and the ability to add markers. It also supports double-tap and pinch to zoom for IOS and Android on mobile phones. Furthermore all the five biggest web browsers are supported, including graceful fallback for old versions.
    
    The most powerful feature of Leaflet is the ability to add layers. The different supported layers are:

    \begin{itemize}
      \item Tile layers
      \item Marker layers
      \item Pop-ups
      \item Vector layers
      \item GeoJSON layers
      \item image overlays
      \item WMS layers
      \item Layer groups
    \end{itemize}

    For our purposes the ability to get map tiles from different sources may be very interesting. This gives us the ability to for example show both nautical maps and regular land maps at the convenience of the user. At zoom levels covering large geographical areas it will probably be most ideal to show the relevant simulated data as overlayed PNGs. This is easily achieved with image overlays in Leaflet. If we want to show very detailed data when zoomed further in, we might be able to use vector layers to visualize the data. It is also possible to use a GeoJSON layer to convert data formatted as GeoJSON to vectors.
    It is also possible to use Web Map Service (WMS) to overlay, for example, metrological data on a map. Eventhough this is a format that is used by large organizations like the National Oceanic and Atsmopheric Administration (\url{noaa.gov}) we have been advised against using this format due to it's negative effect on the speed and responsiveness experienced by the end-user. \footnote{Iván Sánchez Ortega, How to Build Slow Maps - Trondheim, September 24, 2014}
    Furthermore Leaflet can be extended with plugins. These can relatively easily be written in Javascript or an existing plugin can be downloaded and used. A relevant plugin to our needs could for example be heatmap.js (\url{http://www.patrick-wied.at/static/heatmapjs/}).

    \paragraph{Summary}
    Leaflet is very suitable for our needs in respect to creating an interactive map overlayed with visualizations of relevant data created by the SINTEF ocean forecast simulations. It's lightweight(33 kilobytes), made to be compatible with mobile phones and very flexible in possibilities to display data on maps. It's also well documented.

  \subsubsection{OpenLayers}
   \begin{tabular}{|p{4cm}|p{8cm}|}
     \hline
     Home page: & \url{http://openlayers.org/} \\
     \hline
     Service functionality: & A high-performance, feature-packed library for all your mapping needs. \\
     \hline
   \end{tabular}
   \paragraph{Introduction} \indent
   OpenLayers is an open source (provided under the \href{'https://tldrlegal.com/license/bsd-2-clause-license-(freebsd)'}{2-clause BSD License}) JavaScript library for displaying map data in web browsers. It provides an API for building rich web-based geographic applications similar to Google Maps and Bing Maps. The library was originally based on the Prototype JavaScript Framework. Since November 2007 OpenLayers is an Open Source Geospatial Foundation project.
   The current stable version, OpenLayers 3.0, has been released August 29, 2014.
   \paragraph{Features}
   OpenLayers provides support to the following functionalities:
   \begin{description}
     \item[Tile layers] It pulls tiles from OSM, Bing, MapBox, Stamen, MapQuest, and any other XYZ source you can find. OGC mapping services and untiled layers also supported.
     \item[Vector layers] Renders vector data from GeoJSON, TopoJSON, KML, GML, and a growing number of other formats.
     \item[Fast \& Mobile Ready] Mobile support is out of the box, and it is possible to build lightweight custom profiles with just the needed components.
     \item[Cutting Edge \& Easy to Customize] Map rendering leverages WebGL, Canvas 2D, and HTML5. Map styling is controlled with straight-forward CSS.
   \end{description}
   \paragraph{Initial load time}
   The initial load time of the map is ok on a PC. The total loading time of the example (html with map and WMS tiles) took around 4 seconds. The size of the Javascript library (ol.js) is 129 KB, and the first time (without any sort of browser caching) it was fetched in 473 ms.
   \\ \emph{Score: \textbf{Med}}
   \paragraph{Responsiveness}
   It's quite fast when pan, but not so fast when zooming in (438 ms + 2.43 s for DNS lookup of the WMS server in the example) or out (around 400 ms was logged, on average, as the time to get tiles from the server, because there is no need of DNS lookup, but the overall perceived time is around 1s).
   \\ \emph{Score: \textbf{Med}}
   \paragraph{Detail and dynamism}
   The detail of the image is defined by PNG provided by the WMS server. Vector layers are also well drawn. Tile layers are updated on zooming.
   \\ \emph{Score: \textbf{High}}
   \paragraph{Ease of use}
   It is quite easy to implement. The code needed is low, and it is easily convertable into Leaflet format. There are a lot of provided working examples, but it is not well documented yet for the version 3.0 (that has been released very recently: at the end of August 2014). Anyway, the detailed API documentation is provided.
   \\ \emph{Score: \textbf{Med}}
   \paragraph{Summary}
   It is a library used in a lot of applications, dynamic, with a lot of features. On the contrary, it is less lightweight than LeafletJS, not so well documented and not always so fast as experienced by the user, especially in zooming.
   \\ \emph{Overall rating: \textbf{Good}}
  
  \subsubsection{Wind map}
  \emph{Link: \url{http://hint.fm/wind/}} \\%make sure to use //
    Wind map is a personal art project that gets surface wind data from the American government agency National Digital Forecast Database and displays it as moving curved lines on a map. This makes for a very intuivive visualization of the data that give a good general picture of the actual real-world situation. It's zoomable and can pan. By clicking on a specific point on the map you get the wind speed and coordinates of the point. A draw back might be the use on moblie devices which may be suboptimal.
  \\ \emph{Overall rating: \textbf{Good}}

  \subsubsection{Comparison of Leaflet and OpenLayers}
    For the purpose of creating an interactive map we've boiled it down to either using LeafletJS or OpenLayers. As the description of each of the individual libraries above shows, the two are have many of the same capabilities. Both gives us the posibility to overlay visulizations of data in different ways. An reason for choosing Leaflet is the extensive documentation of the current version, a point where OpenLayers are lacking at the moment. Despite this OpenLayers support of WMTS and WMSC is such a strong argument in it's favor that we jugde it as the best fit for our purposes.
    
  \subsection{Godiva2}
     \begin{tabular}{|p{4cm}|p{8cm}|}
     \hline
     Example page: & \url{http://behemoth.nerc-essc.ac.uk/ncWMS/godiva2.html} \\
     \hline
     Service functionality: & A browser client made to browse data served by a ncWMS server. \\
     \hline
   \end{tabular}
   
  \paragraph{Introduction}
  Godiva2 was created as a companion for ncWMS, to display the pngs served by ncWMS as tiles in a map interface. It is a fairly simple html page using JavaScript. \\
   It presents the user with a pannable, zoomable map interface of the earth. Variables (for example ocean temperature) can be selected from a menu. An overlay is added to the map, and the map zooms and pans to show the relevant area. The user can select the date, time of day and depth, updating the map immediately. Clicking on the map brings up a context menu, where a vertical profile plot can be created for that spot. Selecting a tool in the map interface, the user can draw a line on the map, and a plot is created showing the value of the selected variable along that line. There is a menu to select a different WMS, changing the background map and projection of any overlays. The site allows the user to grab a permalink of the current state of the map.

	\paragraph{Load time}
	Before selecting a variable, only a list of available variables and the background map are fetched. Testing Godiva2 on a local ncWMS server the initial load time is low. Tests using publicly available datasets vary, but some have low load time, indicating that the slower ones might just have less available resources.
   \\ \emph{Score: \textbf{Good}}
	
	\paragraph{Responsiveness}
	The responsiveness of panning and zooming the map, as well as changing the desired depth and timestep varies quite a bit. This is true both for local and remote datasets.
   \\ \emph{Score: \textbf{Medium}}
	
	\paragraph{Detail and dynamism}
	Every bit of detail available in the dataset is shown, as required by the level of zoom.
   \\ \emph{Score: \textbf{High}}
	
	\paragraph{Ease of Use}
	The menus for changing variable, depth and timestep are all intuitive and easy to use. The map interface follows the conventions for zooming and panning with the mouse.
   \\ \emph{Score: \textbf{High}}
	
	\paragraph{Summary}
	This is a product made to browse 4 dimensional geospatial data like the ones we want to present. The look and feel is slightly outdated, but the functionality it has works well. It is open source. It and the third party libraries are licensed under free software licenses. It (and the ncWMS server) would need to be modified in order to display all the data and charts found in the current solution.
   \\ \emph{Overall rating: \textbf{Good}}
	

\section{Recommendation}
Based on the study of possible solutions in the previous chapter, the group has made the following recommendation to the customer.

\subsection{Front-end}
As the group has looked into the situation at SINTEF, as well as other solutions in production, the conclusion is rather clear. SINTEF requires a very specific front end, with custom graphs, and the ability to add more functionality at a later stage. As far as existing solutions go, only the Godiva2 system has been close to performing the necessary tasks to serve SINTEFs needs. This system is however not very easy to adapt, and the group is of the opinion that changing this system to fit custom needs may be too much work to justify.

With this in mind, the group feels that the only realistic solution for the front-end is to develop a custom system from scratch.

\subsection{Back-end}
For the back-end, the group is divided in its recommendation. From our investigation into different technologies, it seems like THREDDS is a solution that is generally implemented for working with NetCDF files. Advantages of using this is that there exists documentation, maintenance, and updates. The group has had trouble configuring THREDDS, and has also gotten the impression that SINTEF is not perfectly satisfied with its performance. 

The group feels confident that a custom back-end prototype can be made, and that it will be able to meet the requirements. This will give the customer more flexibility in functionality and expandability, but will obviously not be as complete or well supported as THREDDS.

The group asks SINTEF to make a decision for a back-end solution based on the information in this report.

\subsection{Different paths}
As SINTEF is given a choice for how to proceed, the group sees two paths forward in the project:
\begin{enumerate}
\item THREDDS back-end: Given that SINTEF chooses to use a THREDDS back-end, the group asks SINTEF to provide a configured THREDDS server. The group will focus their work on the front-end, and attempt to make a prototype that replicates most functionality from todays site.
\item Custom back-end: Given that SINTEF chooses a custom back-end, the team will split into two groups, and attempt to make a prototype back-end, as well as a prototype front-end. In this scenario, both the front- and back-end will have limited capabilities, but will demonstrate what is possible with a fully custom solution.
\end{enumerate}


\chapter{Requirements}
This chapter presents the functional and non-functional requirements for the project, as well as system backlog priorities and estimates.

\section{Use Cases}
The desired behaviour of the system can be described through use case diagrams. Here we will display the users and how they want to interact with the system. Use cases model the system requirements and is a easy way to understand which features are needed and how these features should work. This section will just outline the general use cases. For more closely specifications we refer to the Requirements Specification section. %refer to it?

  \subsection{Planning}
  In our project, we have a relatively simple set of users and controll of parametres. The main focus of our task is to deliver a full system with working front-end and back-end modules. In this case the user should be able to use the front-end without further knowledge about the back-end. The user will interact with a GUI (Graphical User Interface) through key-input and mouse-input. Mainly this will be map-interaction like scrolling or dragging the map, but parametres will also be controlled through buttons and other input fields.

  There is also a second type of users that is not as important in our scope, but will have to be considered. In this case the user is a 3rd-party site or company that wants to access the data directly from our back-end for use with their own front-end solution. These users will send requests over REST with commands from an API for allowed requests.

  \subsection{Users}
  Out product will have two types of users.
  \\ \\
  The end-user is a user that uses our front-end solution, which again depends on our back-end solution. Among such users there currently not a goal to differentiate users, but at a later point registration and authentication might be required. These users need a easially usable GUI which feels responsive to the users.
  \\ \\
  The 3rd-party user is a user that only uses the back-end solution of our project. These services will again be used by end-users. All these 3rd-party users might have to be authenticated to use the back-end directly. This is for security-reasons meaning that no requests will be processed if it is not authenticated and within the API of allowed requests.
  \\ \\
  Together these to types of users requires us to have a clear and usable interface both from the back-end and the front-end. While the end-users are the main focus, adapting for the 3rd-party users should be easy as we will consider the usability of the interface between the front-end and back-end for our own communication between the two.

  \subsection{Use Case Diagrams}
  Our two users have a limited set of operations that they can perform, but with a wide range of parameters that can be adjusted. For the end-user these are shown in Figure[\ref{funcReqsBack}] and for the 3rd-party services these are shown in Figure[\ref{funcReqsFront}]
  \begin{figure}[h]
	\begin{center}
	\includegraphics[height=125px,width=186px]{img/useCase_EndUser.png}
	\caption{End-user Use Case Diagram}
	\label{fig:endUserUseDiagram}
	%\medskip
	\small
	\end{center}
  \end{figure}

  \begin{figure}[h]
	\begin{center}
	\includegraphics[height=125px,width=186px]{img/useCase_3rdParty.png}
	\caption{3rd-party Use Case Diagram}
	\label{fig:3rdPartyUseDiagram}
	%\medskip
	\small
	\end{center}
  \end{figure}

    The end-user is doing actions on the front-end part of the system, while the 3rd-party service performs its actions on the back-end part of the system. The end-user would then again use the 3rd-party service. In the future more operations might be allowed as the system expands from its core functionality, but these have been left outside of our scope for this project.

  \subsection{Use cases}
	For our prestudy SINTEF put forth a series of situations were their system is used by the fishery industry. 
	\begin{itemize}
		\item \textbf{Planning of new fish farm sites} \\
		When choosing a new site for a fish farm, it is important to be aware of historical information on currents, temperature and salinity. Placing a new site in an area that has high currents might make construction and other operations difficult.
		\item \textbf{Planning the removal of lice in a fish farm} \\
		In the event that lice has to be removed from the fish in the farm, there are two procedures that are used. In the first scenario, a tanker vessel will pump the fish from the farm into a holding room. Here, the fish is treated with chemicals before being pumped back to the farm. The second method brings in crane boats, that hoist the farm, thus shrinking the volume the fish has available. A canvas is then used to surround the fish, and the chemicals are added directly. 

		Both these operations require calm currents, as to ensure that any support vessels are able to stay in position during the operation. Having access to the latest current predictions is vital in the planning of this type of operation.
		\item \textbf{Planning feeding of fish in fish farms} \\
		The fish in a farm is kept in close confinement. When the fish is fed, their activity rises significantly, consuming larger amounts of oxygen than usual. Given the right combination of temperature and currents, this could lead to oxygen levels dropping to dangerously low levels. Being able to plan feeding in accordance with forecasts for temperature and currents minimizes the risk of harm to the fish.
	\end{itemize}

\section{Requirements Specification}
We have decided to have detailed documentation on our functional requirements, as we choose to base our product backlog from them.

  \subsection{Prioritization}
  All functional requirements have been prioritized on a scale of 1 to 100 in order of importance, where 1 is the lowest priority and 100 is the highest. These priorities where set by the customer based on our input and priorities in the categories High, Medium and Low.

  \subsection{Functional Requirements}
  The functionality of the system is described through its functional requirements. Based on the conversations with the customer, use cases and existing solutions for these use cases it is possible to formulate the functional requirements. Below is a table listing all the functional requirements devided into tables for front-end and back-end. The requirements are listed with deeper explanation, priority, time estimate.
  \\
  \begin{longtable}{p{1.4cm} p{7.8cm} p{1cm} p{1cm} }
  \caption[]{Functional Requirements Back-End}\\
  \multicolumn{1}{p{1.4cm}}{ID} &
  \multicolumn{1}{p{7.8cm}}{Requirement Description} &
  \multicolumn{1}{p{1cm}}{Prio.} &
  \multicolumn{1}{p{1cm}}{Est.}
  \endhead

  \caption[Functional Requirements Back-End]{} \label{funcReqsBack} \\
  \hline \multicolumn{4}{c}{\textbf{Back-End}} \\
  \hline
  \multicolumn{1}{p{1.4cm}}{ID} &
  \multicolumn{1}{p{7.8cm}}{Requirement Description} &
  \multicolumn{1}{p{1cm}}{Prio.} &
  \multicolumn{1}{p{1cm}}{Est.}
  \endfirsthead
  
  \hline
  \multicolumn{4}{r}{{Continued on next page}} \\
  \endfoot

  \hline \hline
  \endlastfoot

  \hline  
  FR 1.1 & The back-end must be able to find the correct files for a given area
  
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given one or more NetCDF files in the storage \\ 
        When a request for an area arrives \\ 
        Then the back-end will return the correct files}
  \end{itemize}
   & 99 & 10h \\ \hline
  
  FR 1.2 & The back-end must be able to find the correct files for a given time 
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given one or more NetCDF files in the storage \\ 
        When a request for a time is given \\ 
        Then the back-end will return the correct files}
  \end{itemize}
  & 98 & 6h \\ \hline %estimage given FR 1.1?

  FR 1.3 & The back-end must be able to find the correct data from a given file 
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a NetCDF file \\ 
        When a request of specific parameters arrives \\ 
        Then the back-end will return the relevant dataset from the NetCDF file}
  \end{itemize}
  & 97 & 8h \\ \hline

  FR 1.4 & The back-end must be able to downsample data to a given resolution 
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a dataset \\ 
        When it is necessary \\ 
        Then the back-end should downsample the data to a reasonable size}
  \end{itemize}
  & 50 & 15h \\ \hline

  FR 1.5 & The back-end must be able to handle overlapping areas and give back correct data
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a set of NetCDF files in store \\
        When a request for an area that is covered by multiple files arrives \\
        Then the back-end should return a single data set with the correct data from all relevant files}
  \end{itemize}
  & 49 & 30h \\ \hline

  FR 1.6 & The back-end must be able to find the most up to date data
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given multiple NetCDF files overlapping in time \\
        When a time in this overlap is requested \\
        Then the data from the most up to date NetCDF file should be used}
  \end{itemize}
  & 48 & 6h \\ \hline

  FR 1.7 & The back-end must be consistent and return the same output for the same input as long as no relevant files have been changed
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a repeated request \\
        When no new relevant files are added \\
        Then the back-end must return the same dataset}
  \item \parbox[t]{6.8cm}{
        Given a repeated request \\
        When new relevant files are added \\
        Then the back-end must return the most up-to-date dataset}
  \end{itemize}
  & 47 & 10h \\ \hline

  FR 1.8 & The back-end must return the most relevant data after new files are added to the system
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a back-end with cached results \\
        When a more up to date NetCDF file is added to the system and request arrives \\
        Then the back-end must calculate new results instead of using the now out of date cached results}
  \end{itemize}
  & 46 & 12h \\ \hline

  FR 1.9 & The back-end should be able to handle new files on a daily basis
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a schedule of adding new files daily \\
        When the new files are added to the system \\
        Then the back-end should process these efficiently without major disturbance to the system}
  \end{itemize}
  & 45 & 8h \\ \hline

  FR 1.10 & Administrators must be able to add files to the system
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a system with an existing set of files \\
        When an administrator wants to add new NetCDF files \\
        Then the back-end should be able to add these to the set of existing files without recompiling}
  \end{itemize}
  & 44 & 6h \\ \hline

  FR 1.11 & Files added to the system should be ready for use without any more manual actions
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a system with an existing set of files \\
        When an administrator has added new NetCDF files \\
        Then the back-end should be able to process these files and make them usable together with the existing set of files without further action from the administrator}
  \end{itemize}
  & 43 & 10h \\ \hline

  FR 1.12 & The back-end should be able to provide correct projections of point data
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a system of NetCDF files \\
        When a point is specified in world coordinates \\
        Then the back-end should be able to correctly project this onto coordinates of the NetCDF files}
  \end{itemize}
  & 42 & 20h \\ \hline

  FR 1.13 & The back-end should be able to provide the current direction data, given a area, time and depth
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a system of files containing current direction data \\
        When a current direction request for a given area, time and depth arrives \\
        Then the back-end should serve the current direction data for this area}
  \end{itemize}
  & 41 & 20h \\ \hline

  FR 1.14 & The back-end should be able to provide the temperature data, given a area, time and depth
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a system of files containing temperature data \\
        When a temperature request for a given area, time and depth arrives \\
        Then the back-end should serve the temperature data for this area}
  \end{itemize}
  & 90 & 15h \\ \hline

  FR 1.15 & The back-end should be able to provide the salinity data, given a area, time and depth
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a system of files containing temperature data \\
        When a temperature request for a given area, time and depth arrives \\
        Then the back-end should serve the temperature data for this area}
  \end{itemize}
  & 89 & 8h given FR 1.14 \\ \hline

  FR 1.16 & The back-end should be able to provide the current data, given a area, time and depth
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a system of files containing current data \\
        When a current request for a given area, time and depth arrives \\
        Then the back-end should serve the current data for this area}
  \end{itemize}
  & 50 & 8h given FR 1.14 \\ \hline

  FR 1.17 & The back-end should be able to provide a PNG image for the given temperature data
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a temperature dataset \\
        When requested to return the dataset in PNG image format \\
        Then the back-end should make a PNG image to represent the dataset with value-color mapping relevant to temperature}
  \end{itemize}
  & 90 & 30h \\ \hline

  FR 1.18 & The back-end should be able to provide a PNG image for the given salinity data
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a salinity dataset \\
        When requested to return the dataset in PNG image format \\
        Then the back-end should make a PNG image to represent the dataset with value-color mapping relevant to salinity}
  \end{itemize}
  & 89 & 10h given FR 1.17 \\ \hline

  FR 1.19 & The back-end should be able to provide a PNG image for the given current data
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a current dataset \\
        When requested to return the dataset in PNG image format \\
        Then the back-end should make a PNG image to represent the dataset with value-color mapping relevant to current}
  \end{itemize}
  & 50 & 10h given FR 1.17  \\ \hline

  FR 1.20 & The back-end should be able to provide current direction data in an easily usable format
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a current direction dataset \\
        When it is not specifically requested to return unprocessed data \\
        Then the back-end should convert the current direction data into an easily usable format like GeoJSON}
  \end{itemize}
  & 50 & 20h \\ \hline

  FR 1.21 & The back-end should provide the raw data in JSON format
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given some data \\
        When requested to return the raw data \\
        Then the back-end should return it in JSON format}
  \end{itemize}
  & 10 & 8h \\ \hline

  FR 1.22 & The back-end should implement the WMTS-protocol & 95 & 30h \\ \hline

  FR 1.23 & The back-end should implement a relevant subset of REST & 10 & 12h \\ \hline
  \end{longtable}
  
  \vspace{2cm}

  \begin{longtable}{p{1.4cm} p{7.8cm} p{1cm} p{1cm} }
  \caption[]{Functional Requirements Front-End} \\
  \hline \hline
  \multicolumn{1}{p{1.4cm}}{ID} &
  \multicolumn{1}{p{7.8cm}}{Requirement Description} &
  \multicolumn{1}{p{1cm}}{Prio.} &
  \multicolumn{1}{p{1cm}}{Est.}
  \endhead

  \caption[Functional Requirements Front-End]{} \label{funcReqsFront} \\
  \hline \multicolumn{4}{c}{\textbf{Back-End}} \\
  \multicolumn{1}{p{1.4cm}}{ID} &
  \multicolumn{1}{p{7.8cm}}{Requirement Description} &
  \multicolumn{1}{p{1cm}}{Prio.} &
  \multicolumn{1}{p{1cm}}{Est.}
  \endfirsthead

  \hline
  \multicolumn{4}{r}{{Continued on next page}} \\
  \endfoot

  \hline \hline
  \endlastfoot
  
  \hline
  FR 2.1 & The front-end must be able to provide a map with overlaying image
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a front-end GUI \\
        When a user loads the page \\
        Then the front-end should provide a map and display an overlaying image on top of it}
  \end{itemize}
  & 100 & 20h \\ \hline

  FR 2.2 & The user must be able to specify area and time for the map to display
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a front-end GUI \\
        When the user want to specify area and time
        Then the user should be able to intuitively do so in the GUI}
  \item \parbox[t]{6.8cm}{
        Given a front-end GUI \\
        When the user has specified area and time
        Then the front-end should provide the data for the given area and time}
  \end{itemize}
  & 90 & 24h \\ \hline

  FR 2.3 & The user must be able to display data from a chosen depth
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a front-end GUI \\
        When the user wants to specify a depth \\
        Then he should be able to intuitively specify the depth in the GUI}
  \item \parbox[t]{6.8cm}{
        Given a front-end GUI \\
        When user specifies a depth \\
        Then the GUI should display the data for the given depth}
  \end{itemize}
  & 80 & 15h \\ \hline

  FR 2.4 & The user must be able to choose data type to be displayed (temperature, salinity or current)
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a front-end GUI \\
        When the user wants to select data type \\
        Then he should be able to intuitively choose the data type in the GUI}
  \item \parbox[t]{6.8cm}{
        Given a front-end GUI \\
        When the user selects a data type \\
        Then the GUI should display the data for this selected type of data in the map}
  \item \parbox[t]{6.8cm}{
        Given a front-end GUI \\
        When the user requests data on temperature, salinity or current \\
        Then it is received and displayed in PNG image format}
  \end{itemize}
  & 70 & 10h \\ \hline

  FR 2.5 & The user must be able to zoom and pan in the displayed map
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a map in the front-end GUI
        When the user wants to zoom or pan
        THen he should be able to do so with intuitive mouse or button actions}
  \end{itemize}
  & 30 & 5h \\ \hline

  FR 2.6 & The existing data must move and scale when zooming and panning
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a map in the front-end GUI with existing data loaded
        When the user performs a zoom or pan action
        Then the existing data should scale and move correctly}
  \end{itemize}
  & 20 & 5h \\ \hline

  FR 2.7 & New data must dynamically load when view-area is changed sufficiently
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a map in the front-end GUI \\
        When the user zoom closer in the map to where a more detailed dataset would be needed \\
        Then the front-end should dynamically load a more detailed dataset}
  \item \parbox[t]{6.8cm}{
        Given a map in the front-end GUI \\
        When the user zoom further away in the map to where not all of the view is covered by the previous dataset \\
        Then the front-end should dynamically load new datasets on the same level of detail, or if a certain threshold is passed it should load datasets on a less detailed level}
  \item \parbox[t]{6.8cm}{
        Given a map in the front-end GUI \\
        When the user pan the map so that we do not have all of the view covered by the previous dataset \\
        Then the front-end should dynamically load the datasets to cover the new view}
  \end{itemize}
  & 10 & 30h \\ \hline

  FR 2.8 & The front-end should be able to display current directions
  \begin{itemize}
  \item \parbox[t]{6.8cm}{
        Given a front-end GUI \\
        When the user requests a dataset and the option for current direction is marked \\
        Then the front-end should request the current direction dataset and display it as an overlay over the map and/or other datasets}
  \end{itemize}
  & 40 & 30h \\ \hline

  \end{longtable}

  \subsection{Non-Functional Requirements}
  We have also made a list of essential non-functional requirements. All of them are essential and will have a high priority in our project.

  \begin{longtable}{p{1.4cm} p{9.8cm} }
  \caption[Non-Functional Requirements]{Non-Functional Requirements} \label{nonFuncReqs} \\
  \hline
  \multicolumn{1}{p{1.4cm}}{\textbf{ID}} &
  \multicolumn{1}{p{9.8cm}}{\textbf{Requirement Description}}
  \endfirsthead

  \multicolumn{2}{r}{{Continued on next page}} \\
  \endfoot

  \hline \hline
  \endlastfoot

  \hline
  QA 1.1 & The system should be easily modifiable to add new functionality\\ \hline

  QA 1.2 & The requests must be fast enough to feel responsive for the user \\ \hline

  QA 1.3 & The modules should be well documented and easily understandable for those who will work on it later \\ \hline

  QA 1.4 & The API between front-end and back-end should be well documented to be usable together with other front-end or back-end solutions \\ \hline

  QA 1.5 & The front-end and back-end parts of the system should be segregated for security and ease of use separately \\ \hline

  \end{longtable}


\chapter{Pre-sprint work}
This chapter will explain how the group worked and what we did in the weeks leading up to Sprint 1.

\section{Understanding the customers problem and needs}

\paragraph{Learning about their current solution}
At our first two meetings with SINTEF we were introduced to how they currently presents data to its users and some of the problems with this solution. They also gave some examples of who their users were and why they might access the data. We were offered access to the front end of their current solution to explore on our own.

\paragraph{Agreeing on the scope of our work}
After some discussion in the group and clarifications from SINTEF, we wrote a suggestion for the scope of our task: 
\begin{quote} Make a pre-study that examines what technology can be used to replace the current collection of PDFs, and instead serve the end user with custom visualisations on a web page. Follow the study with an implementation of a proof of concept system that is able to read a netCDF-file, and present data to the end user based on custom chosen variables. We will ignore any bottlenecks that may exist in SINTEFs systems, and assume that they are fixable. Our end report will contain suggestions for further development and analysis.
\end{quote}
Our contact at SINTEF accepted this with one change, that the system should be able to read from a collection of netCDF files. The group agreed to this change.

\paragraph{Requirements / backlog}
From our formalized scope and the choice the customer made based on our pre study, we created a list of requirements. The list of functional requirements also serves as our project backlog. In discussion with the group, the customer gave each requirement a priority to serve as a guideline when creating the sprint backlogs.

\section{Group organization}
\paragraph{Work-flow}
After some discussion and investigation the group decided to use scrum for the main part of the project. Before starting the scrum sprints proper we assigned tasks less formally. At each meeting we decided on what tasks should be done by next meeting, and divided them among us.

\paragraph{Roles and teams}
Each group member was assigned one or more areas of responsibility. The group was split into two teams to research for the pre study, one for front end and one for back end. This split will remain as the customer wants us to create both a custom front and back end.

\paragraph{Setting up tools}
The group agreed on and set up several tools to facilitate communication and work:
\begin{itemize}
	\item Slack channels for instant messaging and file sharing
	\item Google Groups for group-wide emails
	\item Git for the reports and code
	\item Google Drive for smaller documents and time tracking
	\item Trello for issue tracking
\end{itemize}

\paragraph{Project plan}
We created a project plan to outline the phases of the project. It also includes a schedule for the sprints and milestones.
	
\paragraph{QA rules}
The group agreed with the customer on time limits for scheduling meetings and email responses.

\paragraph{Risk assessment}
In order to be better prepared, we listed and assessed possible problems. We also outlined ways of minimizing the chance of the risk occurring, as well as how they might be handled if they should occur.

\section{Work on the pre study}
The main goal of the first project period was to produce a preliminary study. This presented the customer with possibilities on how the work should proceed. Based on a recommendation by the group and after some discussion, the customer made a decision. The pre study took several weeks to produce, and work focused on these tasks:

\paragraph{Documenting the current solution}
To gain a better perspective on what the customer wanted, we documented what SINTEFs current system looked like, what data it presented and how. This project aims to solve some of the challenges and limitations of the current system.

\paragraph{Researching production solutions}
In order to better understand what a modern system similar to what SINTEF wants might look like we investigated online solutions in production around the world. This also gave some insight into what technologies are most commonly used. Prompting us to look into whether these technologies could be used in our project as well.

\paragraph{Back end technologies}
The back end part of the group looked into and tested several technologies that might be useful. One of the largest problems was finding some technology to handle the aggregation of the different gridsizes in SINTEFs datasets. Our investigations revealed that no existing technology would cover our needs, at least without extensive modification. Other problems included limited support for netCDF and difficulty configuring the programs. We created some prototypes to learn more about how difficult a custom solution from scratch might be, and its performance.

\paragraph{Front end technologies}
Several technologies were available for presenting map data to the user. We looked into rendering the data client-side. This proved inefficient, and together with observations of other production solutions using png-tiles we decided to look more into that instead. Some testing revealed OpenLayers to be the best candidate for handling the dynamic map presentation of data.

\paragraph{Recommendation and decision}
In internal discussions the group found it best to present SINTEF not with one recommendation, but a choice between two paths. For the front end, we concluded that a custom solution would be the best, because what little existed would need a lot of modification. The back-end would either be a THREDDS server configured by SINTEF or a custom solution. The path involving a custom back end came with the caveat that the final product might not be as feature complete, as the group would need to divide its attention.
\par Our contacts at SINTEF were presented with the findings and our recommendation. After some discussions with the group they chose the custom back end path.

\section{Other}

\paragraph{Proof of concept back end}
Early on we knew we there was a possibility of having to create or modify code accessing and serving the data from netCDF files. In order to learn about the performance and effort involved we created some prototypes in Java. We were quickly able to extract data from the netCDF files, serving them as JSON via REST or creating images.


\chapter{Sprint 1}
\section{Sprint planning}
\section{Sprint backlog}
\section{Sprint overview}
\section{Evaluation}

\chapter{Sprint 2}
\section{Sprint planning}
\section{Sprint backlog}
\section{Sprint overview}
\section{Evaluation}

\chapter{Sprint 3}
\section{Sprint planning}
\section{Sprint backlog}
\section{Sprint overview}
\section{Evaluation}

\chapter{Sprint 4}
\section{Sprint planning}
\section{Sprint backlog}
\section{Sprint overview}
\section{Evaluation}

\chapter{Sprint 5}
\section{Sprint planning}
\section{Sprint backlog}
\section{Sprint overview}
\section{Evaluation}

\chapter{Sprint 6}
\section{Sprint planning}
\section{Sprint backlog}
\section{Sprint overview}
\section{Evaluation}

\chapter{Final product}
\section{Description}
\section{Goal completion}
\section{Further work}

\chapter{Evaluation}
\section{Product evaluation}
\section{Team evaluation}
\section{Subject evaluation}


% Sources cited in the document
% uncomment when there are some citations, uncomment bibtex in Makefile
\bibliographystyle{plain}
\chapter{Bibliography}
\begin{flushleft}
	\bibliography{report}
\end{flushleft}

% Appendixes
\appendix

\end{document}
